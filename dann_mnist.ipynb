{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Imports and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Lambda\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Model\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import GradientReversalLayer\n",
    "from keras.engine.training import make_batches\n",
    "from keras.datasets import mnist_m\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "''' This class will build the necessary architecture of a domain adaptation model as \n",
    "proposed by Ganin (2016). Please note that to run this model, you have to update the \n",
    "keras module with the files provided in the keras/ folder.\n",
    "'''\n",
    "class DANNBuilder(object):\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.net = None\n",
    "        self.domain_invariant_features = None\n",
    "        self.grl = None\n",
    "        self.opt = SGD()\n",
    "\n",
    "    def _build_feature_extractor(self, model_input):\n",
    "        '''Build segment of net for feature extraction.'''\n",
    "        net = Convolution2D(nb_filters, nb_conv, nb_conv,\n",
    "                            border_mode='valid',\n",
    "                            activation='relu')(model_input)\n",
    "        net = Convolution2D(nb_filters, nb_conv, nb_conv,\n",
    "                            activation='relu')(net)\n",
    "        net = MaxPooling2D(pool_size=(nb_pool, nb_pool))(net)\n",
    "        net = Dropout(0.5)(net)\n",
    "        net = Flatten()(net)\n",
    "        self.domain_invariant_features = net\n",
    "        return net\n",
    "\n",
    "    def _build_classifier(self, model_input):\n",
    "        net = Dense(128, activation='relu')(model_input)\n",
    "        net = Dropout(0.5)(net)\n",
    "        net = Dense(nb_classes, activation='softmax',\n",
    "                    name='classifier_output')(net)\n",
    "        return net\n",
    "\n",
    "    def build_source_model(self, main_input, plot_model=False):\n",
    "        net = self._build_feature_extractor(main_input)\n",
    "        net = self._build_classifier(net)\n",
    "        model = Model(input=main_input, output=net)\n",
    "        if plot_model:\n",
    "            plot(model, show_shapes=True)\n",
    "        model.compile(loss={'classifier_output': 'categorical_crossentropy'},\n",
    "                      optimizer=self.opt, metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def build_dann_model(self, main_input, plot_model=False):\n",
    "        net = self._build_feature_extractor(main_input)\n",
    "        self.grl = GradientReversalLayer()\n",
    "        branch = self.grl(net)\n",
    "        branch = Dense(128, activation='relu')(branch)\n",
    "        branch = Dropout(0.1)(branch)\n",
    "        branch = Dense(2, activation='softmax', name='domain_output')(branch)\n",
    "\n",
    "        # When building DANN model, route first half of batch (source examples)\n",
    "        # to domain classifier, and route full batch (half source, half target)\n",
    "        # to the domain classifier.\n",
    "        net = Lambda(lambda x: K.switch(K.learning_phase(), x[:int(batch_size / 2), :], x, lazy=True),\n",
    "                     output_shape=lambda x: ((batch_size / 2,) + x[1:]))(net)\n",
    "\n",
    "        net = self._build_classifier(net)\n",
    "        model = Model(input=main_input, output=[branch, net])\n",
    "        if plot_model:\n",
    "            plot(model, show_shapes=True)\n",
    "        model.compile(loss={'classifier_output': 'categorical_crossentropy',\n",
    "                      'domain_output': 'categorical_crossentropy'},\n",
    "                      optimizer=self.opt, metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def build_tsne_model(self, main_input):\n",
    "        '''Create model to output intermediate layer\n",
    "        activations to visualize domain invariant features'''\n",
    "        tsne_model = Model(input=main_input,\n",
    "                           output=self.domain_invariant_features)\n",
    "        return tsne_model\n",
    "\n",
    "\n",
    "def batch_gen(batches, id_array, data, labels):\n",
    "    for batch_index, (batch_start, batch_end) in enumerate(batches):\n",
    "        batch_ids = id_array[batch_start:batch_end]\n",
    "        if labels is not None:\n",
    "            yield data[batch_ids], labels[batch_ids]\n",
    "        else:\n",
    "            yield data[batch_ids]\n",
    "        np.random.shuffle(id_array)\n",
    "\n",
    "\n",
    "def evaluate_dann(X_test, batch_size):\n",
    "    \"\"\"Predict batch by batch.\"\"\"\n",
    "    size = batch_size / 2\n",
    "    num_batches = X_test.shape[0] / size\n",
    "    acc = 0\n",
    "    for i in range(0, num_batches):\n",
    "        _, prob = dann_model.predict_on_batch(X_test[i * size:i * size + size])\n",
    "        predictions = np.argmax(prob, axis=1)\n",
    "        actual = np.argmax(y_test[i * size:i * size + size], axis=1)\n",
    "        acc += float(np.sum((predictions == actual))) / size\n",
    "    return acc / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Processing\n",
    "\n",
    "For images to be used with neural networks, some basic processing has to be done to the images. The data has to be 32bit floats, normalized and be 3D (RGB). The MNIST data used below does not need to be rescaled (is quite low dimensional), but usually normal images need to be downscaled to the ~250px^2 range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Process source data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "# Process target data\n",
    "mnistm = mnist_m.load_data()\n",
    "\n",
    "XT_test = np.swapaxes(np.swapaxes(mnistm[b'test'], 1, 3), 2, 3).astype('float32') / 255\n",
    "XT_train = np.swapaxes(np.swapaxes(mnistm[b'train'], 1, 3), 2, 3).astype('float32') / 255\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "X_train = np.concatenate([X_train, X_train, X_train], axis=1).astype('float32') / 255\n",
    "X_test = np.concatenate([X_test, X_test, X_test], axis=1).astype('float32') / 255\n",
    "\n",
    "# Get domain labels\n",
    "domain_labels = np.vstack([np.tile([0, 1], [int(batch_size / 2), 1]),\n",
    "                           np.tile([1, 0], [int(batch_size / 2), 1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "nb_epoch = 15\n",
    "nb_classes = 10\n",
    "img_rows, img_cols = 28, 28\n",
    "nb_filters = 32\n",
    "nb_pool = 2\n",
    "nb_conv = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "main_input = Input(shape=(3, img_rows, img_cols), name='main_input')\n",
    "src_model = DANNBuilder.build_source_model(main_input)\n",
    "dann_model = DANNBuilder.build_dann_model(main_input)\n",
    "\n",
    "print('Training source only model')\n",
    "src_model.fit(X_train, y_train, batch_size=64, nb_epoch=10, verbose=1,\n",
    "              validation_data=(X_test, y_test))\n",
    "print('Evaluating target samples on source-only model')\n",
    "print('Accuracy: ', src_model.evaluate(XT_test, y_test)[1])\n",
    "\n",
    "# Broken out training loop for a DANN model.\n",
    "src_index_arr = np.arange(X_train.shape[0])\n",
    "target_index_arr = np.arange(XT_train.shape[0])\n",
    "\n",
    "batches_per_epoch = len(X_train) / batch_size\n",
    "num_steps = nb_epoch * batches_per_epoch\n",
    "j = 0\n",
    "\n",
    "print('Training DANN model')\n",
    "\n",
    "for i in range(nb_epoch):\n",
    "\n",
    "    batches = make_batches(X_train.shape[0], batch_size / 2)\n",
    "    target_batches = make_batches(XT_train.shape[0], batch_size / 2)\n",
    "\n",
    "    src_gen = batch_gen(batches, src_index_arr, X_train, y_train)\n",
    "    target_gen = batch_gen(target_batches, target_index_arr, XT_train, None)\n",
    "\n",
    "    losses = list()\n",
    "    acc = list()\n",
    "\n",
    "    print('Epoch ', i)\n",
    "\n",
    "    for (xb, yb) in src_gen:\n",
    "\n",
    "        # Update learning rate as described in the paper.\n",
    "        p = float(j) / num_steps\n",
    "        lr = 0.01 / (1. + 10 * p)**0.75\n",
    "        builder.opt.lr = lr\n",
    "\n",
    "        if xb.shape[0] != batch_size / 2:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            xt = target_gen.next()\n",
    "        except:\n",
    "            # Regeneration\n",
    "            target_gen = target_gen(target_batches, target_index_arr, XT_train,\n",
    "                                    None)\n",
    "\n",
    "        # Concatenate source and target batch\n",
    "        xb = np.vstack([xb, xt])\n",
    "\n",
    "        metrics = dann_model.train_on_batch({'main_input': xb},\n",
    "                                            {'classifier_output': yb,\n",
    "                                            'domain_output': domain_labels},\n",
    "                                            check_batch_dim=False)\n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Evaluating target samples on DANN model')\n",
    "acc = evaluate_dann(XT_test, batch_size)\n",
    "print('Accuracy:', acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
