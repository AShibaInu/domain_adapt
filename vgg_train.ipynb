{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Fog Density Estimation with Convolutional Neural Networks\n",
    "\n",
    "In this notebook I use a convolutional neural network (CNN) for fog density classification from CCTV images. The network is trained on images obtained from KNMI weather stations in De Bilt and Cabauw. \n",
    "\n",
    "Neural networks that are trained from scratch need a lot of data to allow itself to obtain decent weights. For that reason, pre-trained CNN can be very useful. These networks are trained on ImageNet, an image dataset containing 1000 generic classes. The network can then be 'fine-tuned' on the weather data. For this notebook, I will use the InceptionV3 CNN with pre-trained ImageNet weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dropout, Dense, GlobalAveragePooling2D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "from keras.applications import InceptionV3\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "K.set_image_dim_ordering('tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "''' Specify the paths to the data.\n",
    "'''\n",
    "train_dir = '../data/Training/'\n",
    "\n",
    "''' Get image metadata\n",
    "'''\n",
    "meta = pd.read_csv(train_dir + 'ImageDescription2.csv')\n",
    "imgID = meta['image_id'].values\n",
    "y_meta = meta['vis_class'].factorize()\n",
    "y = list(y_meta[0])\n",
    "basenames = meta['basename'].values\n",
    "\n",
    "''' Define functions to load the training images.\n",
    "'''\n",
    "def process_img(path):\n",
    "    return cv2.resize(cv2.imread(path), (img_width, img_height), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "def load_data(data_dir, y):\n",
    "    X = [process_img(glob.glob(os.path.join(data_dir, basename))[0]) for basename in basenames]\n",
    "    X = np.array(X, dtype=np.uint8).transpose((0,1,2,3)).astype('float32') / 255\n",
    "    y = np_utils.to_categorical(np.array(y, dtype=np.uint8), 4)\n",
    "    return X, y\n",
    "\n",
    "''' Load images tailored for network input. \n",
    "However, input dimensions are required to be at \n",
    "least 224x224x3.\n",
    "'''\n",
    "img_width = img_height = 224\n",
    "X, y = load_data(train_dir, y)\n",
    "\n",
    "\n",
    "''' Create stratified train test split\n",
    "'''\n",
    "X_cab, y_cab = X[2517:], y[2517:]\n",
    "X_bil, y_bil = X[:2517], y[:2517]\n",
    "\n",
    "X_trainc, X_testc, y_trainc, y_testc = train_test_split(X_cab, y_cab, test_size=0.2)\n",
    "X_trainb, X_testb, y_trainb, y_testb = train_test_split(X_bil, y_bil, test_size=0.2)\n",
    "\n",
    "X_train = np.append(X_trainc, X_trainb, axis=0)\n",
    "X_test = np.append(X_testc, X_testb, axis=0)\n",
    "y_train = np.append(y_trainc, y_trainb, axis=0)\n",
    "y_test = np.append(y_testc, y_testb, axis=0)\n",
    "\n",
    "del X_trainc, X_trainb, X_testc, X_testb, y_trainc, y_trainb, y_testc, y_testb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading network with pre-trained weights...\n",
      "Fully connected layer training initialized...\n",
      "Train on 3064 samples, validate on 766 samples\n",
      "Epoch 1/2\n",
      "3064/3064 [==============================] - 507s - loss: 1.3401 - acc: 0.3858 - val_loss: 1.1852 - val_acc: 0.4465\n",
      "Epoch 2/2\n",
      "3064/3064 [==============================] - 495s - loss: 1.1938 - acc: 0.4634 - val_loss: 1.0968 - val_acc: 0.5431\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4a42279ef0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Load the network. Pre-trained Imagenet weights are loaded into the models.\n",
    "'''\n",
    "print('Loading network with pre-trained weights...')\n",
    "base = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "\n",
    "'''\n",
    "Define a custom fully connected layer to replace the top layers. This is necessary to\n",
    "support 4 output classes, instead of the default 1000.\n",
    "'''\n",
    "x = base.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(2048, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "preds = Dense(4, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base.input, outputs=preds)\n",
    "\n",
    "'''\n",
    "Because we append an untrained fully connected layer, we have to train it\n",
    "exclusively, i.e.: freeze the weights of the network. Otherwise, the randomly \n",
    "initialized weights of our fully connected layer causes random error to \n",
    "back-propagate into the 'correct' weights.\n",
    "'''\n",
    "for layer in base.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "# Compile the neural network. \n",
    "model.compile(optimizer=SGD(lr=1e-3, decay=1e-6, momentum=.9, nesterov=True),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "'''\n",
    "Train the fully connected layer. Earlier runs have shown that 2 epochs\n",
    "are required before the network starts over-fitting.\n",
    "'''\n",
    "print('Fully connected layer training initialized...')\n",
    "model.fit(x=X_train, y=y_train, validation_split=0.2, epochs=2, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfreezing top-layer weights...\n",
      "Transferlearning top part of network..\n",
      "Train on 3063 samples, validate on 766 samples\n",
      "Epoch 1/100\n",
      "3063/3063 [==============================] - 752s - loss: 1.1304 - acc: 0.5047 - val_loss: 1.0648 - val_acc: 0.5222\n",
      "Epoch 2/100\n",
      "3063/3063 [==============================] - 740s - loss: 1.0980 - acc: 0.5126 - val_loss: 1.0392 - val_acc: 0.5313\n",
      "Epoch 3/100\n",
      "3063/3063 [==============================] - 736s - loss: 1.0449 - acc: 0.5475 - val_loss: 1.0064 - val_acc: 0.5522\n",
      "Epoch 4/100\n",
      "3063/3063 [==============================] - 743s - loss: 1.0069 - acc: 0.5547 - val_loss: 0.9859 - val_acc: 0.5757\n",
      "Epoch 5/100\n",
      "3063/3063 [==============================] - 738s - loss: 0.9683 - acc: 0.5850 - val_loss: 0.9648 - val_acc: 0.5927\n",
      "Epoch 6/100\n",
      "3063/3063 [==============================] - 735s - loss: 0.9302 - acc: 0.6046 - val_loss: 0.9495 - val_acc: 0.6084\n",
      "Epoch 7/100\n",
      "3063/3063 [==============================] - 735s - loss: 0.8828 - acc: 0.6340 - val_loss: 0.9392 - val_acc: 0.6123\n",
      "Epoch 8/100\n",
      "3063/3063 [==============================] - 743s - loss: 0.8629 - acc: 0.6572 - val_loss: 0.9174 - val_acc: 0.6266\n",
      "Epoch 9/100\n",
      "3063/3063 [==============================] - 745s - loss: 0.8264 - acc: 0.6598 - val_loss: 0.9013 - val_acc: 0.6371\n",
      "Epoch 10/100\n",
      "3063/3063 [==============================] - 740s - loss: 0.7957 - acc: 0.6817 - val_loss: 0.8882 - val_acc: 0.6475\n",
      "Epoch 11/100\n",
      "3063/3063 [==============================] - 735s - loss: 0.7820 - acc: 0.6843 - val_loss: 0.8742 - val_acc: 0.6527\n",
      "Epoch 12/100\n",
      "3063/3063 [==============================] - 734s - loss: 0.7403 - acc: 0.6960 - val_loss: 0.8578 - val_acc: 0.6593\n",
      "Epoch 13/100\n",
      "3063/3063 [==============================] - 742s - loss: 0.7199 - acc: 0.7205 - val_loss: 0.8437 - val_acc: 0.6645\n",
      "Epoch 14/100\n",
      "3063/3063 [==============================] - 744s - loss: 0.6964 - acc: 0.7310 - val_loss: 0.8318 - val_acc: 0.6658\n",
      "Epoch 15/100\n",
      "3063/3063 [==============================] - 738s - loss: 0.6814 - acc: 0.7326 - val_loss: 0.8155 - val_acc: 0.6710\n",
      "Epoch 16/100\n",
      "3063/3063 [==============================] - 737s - loss: 0.6504 - acc: 0.7369 - val_loss: 0.8056 - val_acc: 0.6723\n",
      "Epoch 17/100\n",
      "3063/3063 [==============================] - 742s - loss: 0.6405 - acc: 0.7499 - val_loss: 0.7912 - val_acc: 0.6762\n",
      "Epoch 18/100\n",
      "3063/3063 [==============================] - 745s - loss: 0.6051 - acc: 0.7617 - val_loss: 0.7894 - val_acc: 0.6736\n",
      "Epoch 19/100\n",
      "3063/3063 [==============================] - 750s - loss: 0.5883 - acc: 0.7803 - val_loss: 0.7847 - val_acc: 0.6762\n",
      "Epoch 20/100\n",
      "3063/3063 [==============================] - 744s - loss: 0.5706 - acc: 0.7724 - val_loss: 0.7754 - val_acc: 0.6802\n",
      "Epoch 21/100\n",
      "3063/3063 [==============================] - 740s - loss: 0.5387 - acc: 0.7894 - val_loss: 0.7622 - val_acc: 0.6893\n",
      "Epoch 22/100\n",
      "3063/3063 [==============================] - 744s - loss: 0.5248 - acc: 0.7940 - val_loss: 0.7578 - val_acc: 0.6945\n",
      "Epoch 23/100\n",
      "3063/3063 [==============================] - 758s - loss: 0.5348 - acc: 0.7937 - val_loss: 0.7500 - val_acc: 0.6932\n",
      "Epoch 24/100\n",
      "3063/3063 [==============================] - 744s - loss: 0.4868 - acc: 0.8129 - val_loss: 0.7548 - val_acc: 0.6906\n",
      "Epoch 25/100\n",
      "3063/3063 [==============================] - 742s - loss: 0.4740 - acc: 0.8119 - val_loss: 0.7458 - val_acc: 0.6958\n",
      "Epoch 26/100\n",
      "3063/3063 [==============================] - 746s - loss: 0.4624 - acc: 0.8185 - val_loss: 0.7346 - val_acc: 0.6984\n",
      "Epoch 27/100\n",
      "3063/3063 [==============================] - 750s - loss: 0.4523 - acc: 0.8328 - val_loss: 0.7311 - val_acc: 0.6997\n",
      "Epoch 28/100\n",
      "3063/3063 [==============================] - 748s - loss: 0.4319 - acc: 0.8299 - val_loss: 0.7322 - val_acc: 0.7010\n",
      "Epoch 29/100\n",
      "3063/3063 [==============================] - 740s - loss: 0.4155 - acc: 0.8420 - val_loss: 0.7270 - val_acc: 0.7037\n",
      "Epoch 30/100\n",
      "3063/3063 [==============================] - 737s - loss: 0.4071 - acc: 0.8508 - val_loss: 0.7308 - val_acc: 0.7023\n",
      "Epoch 31/100\n",
      "3063/3063 [==============================] - 747s - loss: 0.3840 - acc: 0.8511 - val_loss: 0.7256 - val_acc: 0.7063\n",
      "Epoch 32/100\n",
      "3063/3063 [==============================] - 747s - loss: 0.3809 - acc: 0.8560 - val_loss: 0.7238 - val_acc: 0.7128\n",
      "Epoch 33/100\n",
      "3063/3063 [==============================] - 741s - loss: 0.3490 - acc: 0.8704 - val_loss: 0.7167 - val_acc: 0.7180\n",
      "Epoch 34/100\n",
      "3063/3063 [==============================] - 745s - loss: 0.3374 - acc: 0.8720 - val_loss: 0.7211 - val_acc: 0.7115\n",
      "Epoch 35/100\n",
      "3063/3063 [==============================] - 751s - loss: 0.3234 - acc: 0.8854 - val_loss: 0.7190 - val_acc: 0.7167\n",
      "Epoch 36/100\n",
      "3063/3063 [==============================] - 755s - loss: 0.3274 - acc: 0.8838 - val_loss: 0.7108 - val_acc: 0.7245\n",
      "Epoch 37/100\n",
      "3063/3063 [==============================] - 749s - loss: 0.2958 - acc: 0.8939 - val_loss: 0.7229 - val_acc: 0.7324\n",
      "Epoch 38/100\n",
      "3063/3063 [==============================] - 741s - loss: 0.2756 - acc: 0.9060 - val_loss: 0.7241 - val_acc: 0.7258\n",
      "Epoch 39/100\n",
      "3063/3063 [==============================] - 748s - loss: 0.2755 - acc: 0.9008 - val_loss: 0.7290 - val_acc: 0.7298\n",
      "Epoch 40/100\n",
      "3063/3063 [==============================] - 754s - loss: 0.2663 - acc: 0.9011 - val_loss: 0.7353 - val_acc: 0.7232\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5145624e80>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Now that the fully connected layer trained its weights accordingly,\n",
    "we can start to unfreeze the weights of the network. We will \n",
    "only unfreeze top layers. The bottom layers contain very abstract \n",
    "feature extractors, which are definitely of use for our data. The top \n",
    "layers, however, extract more specific features that were tailored \n",
    "for the data its weights were originally trained from (Imagenet).\n",
    "Therefore, we only want to fine-tune top layer weights, and not bottom \n",
    "weights. We chose to fine-tune the top 2 inception modules.\n",
    "'''\n",
    "print('Unfreezing top-layer weights...')\n",
    "for layer in base.layers[:151]:\n",
    "    layer.trainable = False\n",
    "for layer in base.layers[151:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "'''\n",
    "After the weights have been unfreezed, the model has to be compiled again.\n",
    "This time, we will use SGD with a very low learning rate to fine-tune the\n",
    "model. \n",
    "'''\n",
    "model.compile(optimizer=SGD(lr=1e-4, decay=1e-6, momentum=.9, nesterov=True),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "'''\n",
    "Train the fully connected layer along with the unfreezed part of\n",
    "the network to fine-tune the weights to optimize the \n",
    "extraction of specific features to our data.\n",
    "'''\n",
    "cb = [EarlyStopping(monitor='val_loss', patience=3, verbose=0)]\n",
    "\n",
    "print('Transferlearning top part of network..')\n",
    "model.fit(x=X_train, y=y_train, validation_split=0.2, epochs=100, batch_size=50, callbacks=cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Save/load model\n",
    "\n",
    "Saving the model saves both the architecture and weights of the model. Use this after training, so that the model can instantly be used for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model.save('inception.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('inception.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning done, making predictions..\n",
      "\n",
      "Classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.88      0.85       391\n",
      "          1       0.71      0.75      0.73       304\n",
      "          2       0.75      0.57      0.65       159\n",
      "          3       0.78      0.72      0.75       105\n",
      "\n",
      "avg / total       0.77      0.77      0.77       959\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[344  43   4   0]\n",
      " [ 61 227  13   3]\n",
      " [ 12  37  91  19]\n",
      " [  4  11  14  76]]\n",
      "\n",
      "Accuracy:\n",
      "0.769551616267\n"
     ]
    }
   ],
   "source": [
    "print('Learning done, making predictions..\\n')\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "y_hat = [np.argmax(p) for p in predictions]\n",
    "y_tr = [np.argmax(p) for p in y_test]\n",
    "\n",
    "print('Classification report:')\n",
    "print(classification_report(y_pred=y_hat, y_true=y_tr))\n",
    "print('')\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(y_pred=y_hat, y_true=y_tr))\n",
    "print('')\n",
    "print('Accuracy:')\n",
    "print(accuracy_score(y_pred=y_hat, y_true=y_tr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
