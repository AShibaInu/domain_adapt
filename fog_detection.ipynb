{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, Activation, Dropout, Input, Flatten, Dense, GlobalAveragePooling2D, BatchNormalization, Merge\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "from keras.applications import VGG16, InceptionV3\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "K.set_image_dim_ordering('tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Specify the paths to the data.\n",
    "'''\n",
    "train_dir = '../data/Training/'\n",
    "\n",
    "''' Get image metadata\n",
    "'''\n",
    "meta = pd.read_csv(train_dir + 'ImageDescription.csv')\n",
    "imgID = meta['image_id'].values\n",
    "y_meta = meta['vis_class'].factorize()\n",
    "y = list(y_meta[0])\n",
    "basenames = meta['basename'].values\n",
    "\n",
    "''' Define functions to load the training images.\n",
    "'''\n",
    "def resize_img(path):\n",
    "    return cv2.resize(cv2.imread(path), (img_width, img_height), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "def load_data(data_dir, y):\n",
    "    X = [resize_img(glob.glob(os.path.join(data_dir, basename))[0]) for basename in basenames]\n",
    "    X = np.array(X, dtype=np.uint8).transpose((0,1,2,3)).astype('float32') / 255\n",
    "    y = np_utils.to_categorical(np.array(y, dtype=np.uint8), 4)\n",
    "    return X, y\n",
    "\n",
    "''' Load images tailored for InceptionV3 input. \n",
    "However, input dimensions are required to be at \n",
    "least 299x299x3.\n",
    "'''\n",
    "img_width = img_height = 299\n",
    "X_inc, y = load_data(train_dir, y)\n",
    "\n",
    "''' Idem for VGG16 input. \n",
    "However, input dimensions are required to be at \n",
    "least 224x224x3.\n",
    "'''\n",
    "img_width = img_height = 224\n",
    "X_vgg, y = load_data(train_dir, y)\n",
    "\n",
    "''' Create a test split.\n",
    "More or less 20% of the data.\n",
    "'''\n",
    "X_inc_val = X_inc[-900:]\n",
    "X_vgg_val = X_vgg[-900:]\n",
    "y_val = y[-900:]\n",
    "\n",
    "X_inc = X_inc[:-900]\n",
    "X_vgg = X_vgg[:-900]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Load the VGG16 and InceptionV3 ConvNets. Pre-trained Imagenet weights are loaded\n",
    "into the models.\n",
    "'''\n",
    "print('Loading VGG16 and InceptionV3 with pre-trained weights...')\n",
    "vgg = VGG16(weights='imagenet')\n",
    "inception = InceptionV3(weights='imagenet')\n",
    "\n",
    "\n",
    "'''\n",
    "This removes the top-most layer of the VGG16 network.\n",
    "Note that the function has to be runned multiple times\n",
    "until the flatten layer has been reached.\n",
    "'''\n",
    "print('Popping layers...')\n",
    "vgg.layers.pop()\n",
    "vgg.layers.pop()\n",
    "vgg.layers.pop()\n",
    "\n",
    "'''\n",
    "Idem.\n",
    "'''\n",
    "inception.layers.pop()\n",
    "\n",
    "'''\n",
    "Define the fully connected layer which merges the flatten layers of\n",
    "both the VGG16 and InceptionV3 networks.\n",
    "'''\n",
    "stacked_model = Sequential()\n",
    "stacked_model.add(Merge([vgg, inception], mode='concat', concat_axis=1))\n",
    "stacked_model.add(Dense(2048))\n",
    "stacked_model.add(Activation('relu'))\n",
    "stacked_model.add(Dropout(0.5))\n",
    "stacked_model.add(Dense(512))\n",
    "stacked_model.add(Activation('relu'))\n",
    "stacked_model.add(Dropout(0.5))\n",
    "stacked_model.add(Dense(4))\n",
    "stacked_model.add(Activation('softmax'))\n",
    "\n",
    "'''\n",
    "Because we append an untrained fully connected layer, we have to train it\n",
    "exclusively, i.e.: freeze the weights of VGG16 and InceptionV3. Otherwise,\n",
    "the random initialized weights of our fully connected layer causes random\n",
    "error to back-propagate into the 'correct' VGG16 and InceptionV3 weights.\n",
    "'''\n",
    "for layer in inception.layers:\n",
    "    layer.trainable = False\n",
    "for layer in vgg.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the stacked neural network. \n",
    "stacked_model.compile(optimizer='rmsprop',\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "'''\n",
    "Train the fully connected layer. Earlier runs have shown that 5 epochs\n",
    "are required before the network starts over-fitting.\n",
    "'''\n",
    "print('Fully connected layer training initialized...')\n",
    "stacked_model.fit(x=[X_vgg, X_inc], y=y, validation_data=([X_vgg_val, X_inc_val], y_val), nb_epoch=3, batch_size=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Now that the fully connected layer trained its weights accordingly,\n",
    "we can start to unfreeze the weights of the VGG16 and InceptionV3 \n",
    "networks. We will only unfreeze top layers. The bottom layers contain\n",
    "very abstract feature extractors, which are definitely of use for our \n",
    "data. The top layers, however, extract more specific features that were\n",
    "tailored for the data its weights were originally trained from (Imagenet).\n",
    "Therefore, we only want to fine-tune top layer weights, and not bottom \n",
    "weights. We chose to fine-tune the top 2 convolutional blocks of VGG16\n",
    "(block 4 and 5), and the top 2 inception modules.\n",
    "'''\n",
    "print('Unfreezing top-layer weights...')\n",
    "for layer in inception.layers[:172]:\n",
    "    layer.trainable = False\n",
    "for layer in inception.layers[172:]:\n",
    "    layer.trainable = True\n",
    "for layer in vgg.layers[:10]:\n",
    "    layer.trainable = False\n",
    "for layer in vgg.layers[10:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "'''\n",
    "After the weights have been unfreezed, the model has to be compiled again.\n",
    "This time, we will use SGD with a very low learning rate to fine-tune the\n",
    "model. \n",
    "'''\n",
    "stacked_model.compile(optimizer=SGD(lr=1e-4, decay=1e-6, momentum=.9, nesterov=True),\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "\n",
    "'''\n",
    "Train the fully connected layer along with the unfreezed part of\n",
    "VGG16 and InceptionV3 to fine-tune the weights to optimize the \n",
    "extraction of specific features to our data.\n",
    "'''\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=7, verbose=0)]\n",
    "\n",
    "print('Transferlearning top part of VGG16 and InceptionV3 in parallel...')\n",
    "stacked_model.fit(x=[X_vgg, X_inc], y=y, validation_data=([X_vgg_val, X_inc_val], y_val), \n",
    "                  nb_epoch=100, batch_size=50, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Learning done, making predictions..')\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "print('Classification report:')\n",
    "print(classification_report(y_pred=[np.argmax(p) for p in pred], y_true=[np.argmax(p) for p in y_test]))\n",
    "print('')\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(y_pred=[np.argmax(p) for p in pred], y_true=[np.argmax(p) for p in y_test]))\n",
    "print('')\n",
    "print('Accuracy:')\n",
    "print(accuracy_score(y_pred=[np.argmax(p) for p in pred], y_true=[np.argmax(p) for p in y_test]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
